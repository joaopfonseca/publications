\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

% or ?
\title{Geometric SMOTE for Imbalanced Datasets with Nominal and Continuous Features}

\author{%
	Joao Fonseca\(^{1*}\), Fernando Bacao\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de
    Campolide, 1070--312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{%
	a4paper,
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\definecolor{hypecol}{HTML}{0875b7}
\hypersetup{%
    colorlinks,
    linkcolor={hypecol},
    citecolor={hypecol},
    urlcolor={hypecol}
}

\begin{document}

\maketitle

\begin{abstract}
    This is an abstract.
\end{abstract}

\section{Introduction}~\label{sec:introduction}

% the problem of imbalanced learning

% existing approaches to address imbalanced learning

% prevalence of categorical features in practical classification settings

% why encoding features with more sophisticated approaches is not a
% straightforward task

% why typical methods cannot be used on datasets with mixed data types

% oversampling (and resampling in general) on datasets with mixed data types
% is a largely unexplored problem

% the proposed method

% structure of the paper

Synthetic Minority Oversampling Technique (SMOTE)~\cite{Chawla2002}.

\section{Related Work}~\label{sec:related_work}

This study focuses on multiclass classification problems. A classification
problem contains $n$ classes, having $C_{maj}$ as the set of majority class
observations (\textit{i.e.}, observations belonging to the most common target
class) and $C_{min}$ as the set of minority class observations (\textit{i.e.},
observations belonging to the least common target class). Typically, an
oversampling algorithm will generate synthetic data in order to ensure
$|C_{min}'|=|C_{maj}|=|C_i|, i \in \{1, \ldots, n\}$.

Since the proposal of SMOTE, several other methods were built upon SMOTE to
improve the quality of the data generated. The process of generating synthetic
data using SMOTE-based algorithms can be divided into two distinct phases
\textbf{[CITATION]}:

\begin{enumerate}
    \item Data selection. A synthetic observation, $x^s$, is generated based
        on two existing observations. A SMOTE-based algorithm employs a given
        heuristic to select a non-majority class observation as the center
        observation, $x^c$, and one of its nearest neighbors, $x^{nn}$,
        selected randomly. For the case of SMOTE, $x^c$ is randomly selected
        from each non-majority class.
    \item Data generation. Once $x^c$ and $x^{nn}$ have been selected, $x^s$
        is generated based on a transformation between the two selected
        observations. In the case of SMOTE, this transformation is 
        a linear interpolation between the two obervations: $x^s = \alpha x^c
        + (1-\alpha) x^{nn}, \alpha \sim \mathcal{U}(0, 1)$.
\end{enumerate}

Modifications to the SMOTE algorithm can be distinguished according to the
phase where the modifications were applied. This distinction is especially
relevant for the case of oversampling on datasets with mixed data types, since
it raises the challenge of computing meaningful distances and k-nearest
neighbors among observations. For example, State-of-the-art oversampling
methods, such as Borderline-SMOTE~\cite{han2005borderline},
ADASYN~\cite{he2008adasyn}, K-means SMOTE~\cite{douzas2018improving} and
LR-SMOTE~\cite{liang2020lr} modify the data selection mechanism and show
promising results in imbalanced learning~\cite{fonseca2021improving}. However,
all of these algorithms select $x^c$ using procedures that include calculating
each observation's k-nearest neighbors or using clustering methods, none of
which is prepared to handle categorical data.

Modifications to SMOTE's generation mechanism are less common. A few
oversampling methods, such as Geometric-SMOTE~\cite{douzas2019geometric}
achieve such modification and have shows promising results in previous
research~\cite{douzas2019imbalanced}. However, this method is also unable to
handle datasets with categorical data. This limitation is especially true for
methods combining modifications in the selection and generation mechanisms, as
is the case of the Geometric Self-Organizing Maps Oversampling
algorithm~\cite{douzas2021g}.

As discussed in Section~\ref{sec:introduction}, research on resampling methods
with mixed data types is scarce. The original paper proposing SMOTE also
proposed SMOTE for Nominal and Continuous (SMOTENC), an adaptation of SMOTE
handle datasets with nominal and continuous features~\cite{Chawla2002}. To
determine the k-nearest neighbors of $x^c$, the nominal features encoded by
multiplying the one-hot encoded categorical features by the median of the
standard deviations of the continuous features. Once $x^c$ and $x^{nn}$ have
been determined, the values of the continuous features in $x^s$ are generated
using the SMOTE generation mechanism, while the categorical features are given
the most common values occurring in the k-nearest neighbors.

Alternatively to SMOTE-based methods, some non-informed over and undersampling
methods may also be used for datasets with nominal and continuous features,
specifically Random Oversampling (ROS) and Random Undersampling (RUS). These
methods consist in randomly duplicating minority class observations (in the
case of ROS), which can lead to overfitting~\cite{park2021combined,
batista2004study}, or randomly removing majority class observations (in the
case of RUS), which may lead to underfitting~\cite{bansal2021analysis}.

Recently a new SMOTE-based oversampling method for datasets with mixed data
types, SMOTE-ENC~\cite{mukherjee2021smote}, was proposed. This method modifies
the encoding mechanism for categorical features

\section{Motivation}~\label{sec:motivation}

$C_{maj}$ set of majority class observations (most common class found in the
target variable)

$C_{min}$ set of minority class observations (least common class found in the
target variable)


\section{Proposed Method}~\label{sec:proposed_method}

\section{Methodology}~\label{sec:methodology}

This section describes how the evaluation of G-SMOTENC was performed.  We
describe the datasets used in the experiment, their source and preprocessing
steps carried out in Section~\ref{sec:experimental_data}. We describe the
resampling and classifications methods used for comparing the performance of
G-SMOTENC with other relevant oversampling and undersampling mthods in
Section~\ref{sec:ml_algorithms}. The performance metrics used are defined in
Section~\ref{sec:performance_metrics}. Finally, the experimental procedure is
described in Section~\ref{sec:experimental_procedure}.

\subsection{Experimental Data}~\label{sec:experimental_data}

The datasets used in this experiment were extracted from the
\href{https://archive.ics.uci.edu}{UC Irvine Machine Learning Repository}. All
of the datasets are publicly available and cover a range of different domains.
The selection of datasets was done to ensure that all datasets are imbalanced
and contained non-metric features (\textit{i.e.}, whether ordinal, nominal or
binary). These datasets will be used to show how the performance of different
classifiers varies according to the used over/undersampling method.

At an initial stage, all datasets were preprocessed manually with minimal
manipulations, to avoid the application of preprocessing methods beyond the
scope of this paper. This step was conducted to remove features and/or
observations with missing values and identifying the non-metric features. The
second stage of our preprocessing was done systematically. The resulting
datasets are shown in Table~\ref{tbl:datasets_description}.

\input{../analysis/datasets_description.tex}

The second part of the data preprocessing pipeline starts with the generation
of artificially imbalanced datasets with different Imbalance Ratios
($IR=\frac{|C_{maj}|}{|C_{min}|}$). For
each original dataset, we create its more imbalanced versions at intervals of
10, while ensuring that $|C_{min}| \ge 15$. The sampling strategy was
determined for class $n \in \{1,\ldots,n,\ldots,m\}$ as a linear interpolation using $|C_{maj}|$ and
$|C_{min}'|=\frac{|C_{maj}|}{IR_{new}}$, as shown in equation~\ref{eq:sampling}.

\begin{equation}~\label{eq:sampling}
    |C_i|^{imb} =
    \min(\frac{|C_{min}'|-|C_{maj}|}{n-1}.|C_i|+|C_{max}|, |C_i|)
\end{equation}

The new, artificially imbalanced dataset, is formed by sampling observations
without replacement from each $C_i$ such that $C_i' \subseteq C_i , |C_i'| =
|C_i|^{imb}$. The artificially imbalanced datasets are marked with its
imbalance ratio as a suffix in Table~\ref{tbl:datasets_description}.

The datasets (both original and artificially imbalanced versions) are then
filtered to ensure all datasets have a minimum of 500 observations.  The
remaining datasets whose number of observations is larger than 5000 are
randomly sampled to match this number of observations. Afterwards, for each
remaining dataset we remove all observations from target classes whose
frequency is lower than 15 observations. Finally, the continuous and discrete
features are scaled to the range $[0,1]$ to ensure a common range between all
features. 

\subsection{Machine Learning Algorithms}~\label{sec:ml_algorithms}

The choice of classifiers used in the experimental procedure were based on
their type (tree-based, nearest neighbors-based, linear model and
ensemble-based), popularity and consistency in performance. We used Decision
Tree (DT), a K-Nearest Neighbors (KNN) classifier, a Logistic
Regression (LR) and a Random Forest (RF).

Given the lack of existing oversamplers that address imbalanced learning
problems with mixed data types, the amount of benchmark methods used is also
limited. We used the well known methods that are compatible with this type of
datasets: SMOTENC, Random Undersampling (RUS) and Random Oversampling (ROS).
Table~\ref{tbl:grid} shows the hyperparameters used for the parameter search
described in Section~\ref{sec:experimental_procedure}.

\begin{table}
	\centering
    \caption{\label{tbl:grid}
        Hyperparameter definition for the classifiers and resamplers used in
        the experiment.
    }
	\begin{tabular}{lll}
		\toprule
		Classifier      &                                  &                                \\
		\midrule
        DT              & min.\ samples split              & 2                              \\
                        & criterion                        & gini                           \\
                        & max depth                        & 3, 6                           \\
		LR              & maximum iterations               & 10000                          \\
                        & multi-class                      & One-vs-All                     \\
		                & solver                           & saga                           \\
                        & penalty                          & None, L1, L2                   \\
		KNN             & \# neighbors                     & 3, 5                           \\
                        & weights                          & uniform                        \\
                        & metric                           & euclidean                      \\
		RF              & min.\ samples split              & 2                              \\
		                & \# estimators                    & 50, 100                        \\
		                & Max depth                        & 3, 6                           \\
                        & criterion                        & gini                           \\
		\toprule
		Resampler       &                                  &                                \\
		\midrule
		SMOTENC         & \# neighbors                     & 3, 5                           \\
		G-SMOTENC       & \# neighbors                     & 3, 5                           \\
                        & deformation factor               & 0.0, 0.25, 0.5, 0.75, 1.0      \\
                        & truncation factor                & -1.0, -0.5, 0.0, 0.5, 1.0      \\
                        & selection strategy               & ``combined'',
                        ``minority'', ``majority''\\
		RUS             & replacement                      & False                          \\
		ROS             & (no applicable parameters)       &                                \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Performance Metrics}~\label{sec:performance_metrics}

Although the typical performance metrics, \textit{e.g.}, Overall Accuracy
(OA), are intuitive to interpret, they are often inappropriate to measure a
classifier's performance in an imbalanced learning context
\textbf{[CITATION]}. For example, to estimate an event that occurs in 1\% of
the dataset, a constant classifier would obtain an OA of 0.99 and still be
unusable. However, this metric is still reported in some of our results to
maintain a metric that is easier to interpret.

More recent surveys have found the Geometric-mean ($\textit{G-mean} =
\sqrt{\overline{Sensitivity} \times \overline{Specificity}}$), F1-score
($\textit{F-score}=2\times\frac{\overline{Precision} \times
\overline{Recall}}{\overline{Precision} + \overline{Recall}}$), $Sensitivity =
\frac{TP}{FN+TP}$ and $Specificity = \frac{TN}{TN + FP}$ to be commonly used
performance metrics in imbalanced learning contexts~\cite{rout2018handling}.
These metrics are calculated as a function of the number of False/True
Positives (FP and TP) and False/True Negatives (FN and TN), having
$Precision = \frac{TP}{TP+FP}$ and $Recall = \frac{TP}{TP+FN}$.
This finding is consistent with other well-known recommendations on the usage
of performance metrics~\cite{jeni2013facing}. This led us to adopt, along with
OA, both F-score and G-mean as the main performance metrics for this study. 

\subsection{Experimental Procedure}~\label{sec:experimental_procedure}

The experimental procedure was applied similarly to all combinations of
resamplers, classifiers and hyperparameter combinations across all datasets.
The evaluation of the models' performance was tested using a 5-fold Cross
Validation (CV) approach. The mean performance in the test set is calculated
over the 5 folds and 3 different runs of the experimental procedure for each
combination resampling/classifier hyperparameters. For each dataset, results
of the hyperparameters that optimize the performance of a resampler/classifier
are selected. These results were then used for analysis and are shown in
Table~\ref{tbl:wide_optimal} (see Appendix).
Figure~\ref{fig:experimental_procedure} shows a diagram of the experimental
procedure described.

\begin{figure}
	\centering
	\includegraphics[width=.8\linewidth]{../analysis/experimental_procedure}
    \caption{Experimental procedure used in this study.
    }~\label{fig:experimental_procedure}
\end{figure}

A CV run consists of a stratified partitioning (\textit{i.e.}, each partition
contains the same relative frequencies of target labels) of the dataset into
five parts. A given resampler/classifier combination with a specific set of
hyperparameters is fit and tested five times, using one of the partitions as a
test set and the remaining ones as training set. The estimated performance
consists of the average classification performance across the five different
test sets. 

\subsection{Software Implementation}~\label{sec:software_implementation}

The algorithmic implementation of G-SMOTENC was written using the Python
programming language and is available in the open-source package
\href{https://github.com/joaopfonseca/ml-research}{ML-Research}~\cite{fonseca2021increasing},
along with other utilities used to produce the experiment and outputs used in
Section~\ref{sec:results_and_discussion}. In addition, the packages
\href{https://github.com/scikit-learn/scikit-learn/}{Scikit-Learn}~\cite{scikit-learn},
\href{https://github.com/scikit-learn-contrib/imbalanced-learn}{Imbalanced-Learn}~\cite{JMLR:v18:16-365}
and \href{https://github.com/georgedouzas/research-learn/}{Research-Learn}
were also used in the experimental procedure to get the implementations of the
classifiers, benchmark over/undersamplers and run the experimental procedure.
The Latex code, Python scripts (including data pulling and preprocessing,
experiment setup and results' analysis), as well as the datasets used are
available in this \href{https://github.com/joaopfonseca/publications}{GitHub
repository}.
 

\section{Results and Discussion}~\label{sec:results_and_discussion}

In this section we present the experimental results. We focus on the
comparison of classification performance using oversamplers whose generation
mechanism is compatible with datasets containing both continuous and
categorical features.

The analysis of our experimental results were developed in two stages: (1)
analysis of mean ranking and absolute performance and (2) statistical
analysis. In Section~\ref{sec:discussion} we discuss the main insights
extracted by analysing the results reported in Sections~\ref{sec:results}
and~\ref{sec:statistical_analysis}.

\subsection{Results}~\label{sec:results}

Table~\ref{tbl:mean_sem_ranks} presents the mean rankings of cross validation
scores across the different combinations of oversamplers, metrics and
classifiers. These results were calculated by assigning a ranking score for
each oversampler from 1 (best) to 4 (worst) for each dataset, metric and
classifier, based on the results reported in Table~\ref{tbl:wide_optimal} (see
Appendix).

\input{../analysis/mean_sem_ranks.tex}

Table~\ref{tbl:mean_sem_scores} presents the mean cross validation scores.
With exception to the OA metric, G-SMOTENC either outperformed or matched the
the remaining oversamplers.

\input{../analysis/mean_sem_scores.tex}

\subsection{Statistical Analysis}~\label{sec:statistical_analysis}

To conduct an appropriate statistical analysis in an experiment with multiple
datasets, it is necessary to use methods that account for the multiple
comparison problem. Based on the recommendations found in~\cite{Demsar2006},
we applied the Friedman test along with the Holm-Bonferroni test for a
post-hoc analysis.

In Section~\ref{sec:performance_metrics} we
explained that OA, although easily interpretable, is not an appropriate
performance metric for imbalanced learning problems. Therefore, the
statistical analysis was developed using the two imbalance-appropriate metrics
used in the study: F-Score and G-Mean. The statistical analysis started with
the assessment of a statistically significant difference in performance across
resampling methods using a Friedman test~\cite{friedman1937use}. The results
of this test are shown in Table~\ref{tbl:friedman_test}. The null hypothesis
is rejected in all cases.

\input{../analysis/friedman_test.tex}

We performed a Holm-Bonferroni test to understand whether the difference in
performance of G-SMOTENC is statistically significant to the remaining
resampling methods. The results of this test are shown in
Table~\ref{tbl:holms_test}. The null hypothesis was rejected in 27 out of 32 tests.

\input{../analysis/holms_test.tex}

\subsection{Discussion}~\label{sec:discussion}

The results reported in Section~\ref{sec:results} show that\ldots

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{../analysis/consistency_analysis_plot}
    \caption{Average ranking of oversamplers over different characteristics of
        the datasets used in the experiment. Legend: IR --- Imbalance Ratio,
        Classes --- Number of classes in the dataset, M/NM ratio --- ratio
        between the number of metric and non-metric features, E$($F-Score$)$
        --- Mean F-Score of dataset across all combinations of classifiers and
        oversamplers.
    }~\label{fig:consistency_analysis}
\end{figure}

\section{Conclusion}~\label{sec:conclusion}

This is a conclusion.

\bibliography{references}
\bibliographystyle{ieeetr}

\appendix

\section{Appendix}

\input{../analysis/wide_optimal.tex}

\end{document}
