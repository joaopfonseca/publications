\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

% or ?
\title{Geometric SMOTE for Imbalanced Datasets with Nominal and Continuous Features}

\author{%
	Joao Fonseca\(^{1*}\), Fernando Bacao\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de
    Campolide, 1070--312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{%
	a4paper,
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\definecolor{hypecol}{HTML}{0875b7}
\hypersetup{%
    colorlinks,
    linkcolor={hypecol},
    citecolor={hypecol},
    urlcolor={hypecol}
}

\begin{document}

\maketitle

\begin{abstract}
    This is an abstract.
\end{abstract}

\section{Introduction}~\label{sec:introduction}

% the problem of imbalanced learning
There are various Machine Learning (ML) tasks that deal with highly imbalanced
datasets, such as fraud transactions detection, fault detection and medical
diagnosis~\cite{tyagi2020sampling}. In these situations, predicting false
positives is often a more acceptable error, since the class of interest is
often the minority class~\cite{vuttipittayamongkol2021class}. However,
standard ML classifiers induce a bias in favor of the classes with highest
frequency and limits the predictive power on lower frequency
classes~\cite{lopez2013insight, das2018handling}. This effect is known, in the
ML community, as Imbalanced Learning. 

% existing approaches to address imbalanced learning
Imbalanced learning involves a dataset with two or more target classes with
uneven class frequencies, where the minority class is defined as the class
with the least amount of observations and the majority is the class with the
highest amount of observations \textbf{[CITATION]}. There are 3 main
approaches to deal with imbalanced learning~\cite{fernandez2013analysing}: (1)
Cost-sensitive solutions attribute a higher misclassification costs to
minority class observations to minimize higher cost errors, (2) Algorithmic
level solutions modify ML classifiers to improve the learning of the minority
class and (3) Resampling solutions generate synthetic minority class
observations and/or remove majority class observations to balance the training
dataset.

Since it is an external approach to imbalanced learning, the later method
becomes particularly useful. It dismisses the required domain knowledge to
build cost matrix and the technical complexity/knowledge of applying an
imbalanced learning-specific classifier. Resampling can be done via
undersampling, oversampling or hybrid approaches~\cite{tarekegn2021review}. In
this paper, we will focus on oversampling approaches.

% prevalence of categorical features in practical classification settings
Currently, the presence of nominal features in imbalanced learning tasks limit
the options available to deal with class imbalance. Even though it is possible
to use encoding methods such as one-hot or ordinal encoding to convert nominal
features into numerical, applying a distance metric on nominal data is
questionable since the data is unordered~\cite{lumijarvi2004comparison}. In
this case, one possible approach is to use models that are able to handle
different scales (\textit{e.g.}, Decision Tree). However, this assumption may
be limitative since there are few ML algorithms where this condition is
verified. Another possible approach is transforming the variables to meet
scale assumptions~\cite{lumijarvi2004comparison}. This has been
explored in the Synthetic Minority Oversampling Technique for Nominal and
Continuous features (SMOTENC)~\cite{Chawla2002} (explained in
Section~\ref{sec:related_work}).

% % why typical methods cannot be used on datasets with mixed data types
% In the presence of datasets with mixed-data types, the usage of most
% well-known resampling algorithms becomes unfeasible.

% oversampling (and resampling in general) on datasets with mixed data types
% is a largely unexplored problem

% the proposed method

% structure of the paper
The rest of this paper is structured as follows:
Section~\ref{sec:related_work} describes the related work and its limitations,
Section~\ref{sec:motivation} discusses the gaps in the current
State-of-the-art this paper aims to address, Section~\ref{sec:proposed_method}
describes the proposed method (G-SMOTENC), Section~\ref{sec:methodology} lays
out the methodology used to test G-SMOTENC,
Section~\ref{sec:results_and_discussion} shows and discusses the results
obtained in the experiment and Section~\ref{sec:conclusion} presents the
conclusions drawn from this study.


\section{Related Work}~\label{sec:related_work}

A classification problem contains $n$ classes, having $C_{maj}$ as the set of
majority class observations (\textit{i.e.}, observations belonging to the most
common target class) and $C_{min}$ as the set of minority class observations
(\textit{i.e.}, observations belonging to the least common target class).
Typically, an oversampling algorithm will generate synthetic data in order to
ensure $|C_{min}'|=|C_{maj}|=|C_i|, i \in \{1, \ldots, n\}$.

Since the proposal of SMOTE, several other methods were built upon SMOTE to
improve the quality of the data generated. The process of generating synthetic
data using SMOTE-based algorithms can be divided into two distinct phases
\textbf{[CITATION]}:

\begin{enumerate}
    \item Data selection. A synthetic observation, $x^{gen}$, is generated based
        on two existing observations. A SMOTE-based algorithm employs a given
        heuristic to select a non-majority class observation as the center
        observation, $x^c$, and one of its nearest neighbors, $x^{nn}$,
        selected randomly. For the case of SMOTE, $x^c$ is randomly selected
        from each non-majority class.
    \item Data generation. Once $x^c$ and $x^{nn}$ have been selected, $x^{gen}$
        is generated based on a transformation between the two selected
        observations. In the case of SMOTE, this transformation is 
        a linear interpolation between the two obervations: $x^{gen} = \alpha x^c
        + (1-\alpha) x^{nn}, \alpha \sim \mathcal{U}(0, 1)$.
\end{enumerate}

Modifications to the SMOTE algorithm can be distinguished according to the
phase where the modifications were applied. This distinction is especially
relevant for the case of oversampling on datasets with mixed data types, since
it raises the challenge of computing meaningful distances and k-nearest
neighbors among observations. For example, State-of-the-art oversampling
methods, such as Borderline-SMOTE~\cite{han2005borderline},
ADASYN~\cite{he2008adasyn}, K-means SMOTE~\cite{douzas2018improving} and
LR-SMOTE~\cite{liang2020lr} modify the data selection mechanism and show
promising results in imbalanced learning~\cite{fonseca2021improving}. However,
all of these algorithms select $x^c$ using procedures that include calculating
each observation's k-nearest neighbors or clustering methods, none of which is
prepared to handle categorical data.

Modifications to SMOTE's generation mechanism are less common. A few
oversampling methods, such as Safe-level SMOTE~\cite{bunkhumpornpat2009safe}
and Geometric-SMOTE~\cite{douzas2019geometric} achieve such modifications and
have shown promising results~\cite{douzas2019imbalanced}. However, these
methods is also unable to handle datasets with categorical data. This
limitation is especially true for methods combining modifications in the
selection and generation mechanisms, as is the case of the Geometric
Self-Organizing Maps Oversampling algorithm~\cite{douzas2021g}. Other methods
attempt to replace the SMOTE data generation mechanism altogether using
different Generative Adversarial Networks (GAN)
architectures~\cite{salazar2021generative, koivu2020synthetic, jo2022obgan}.
However, these models are not only computationally expensive to train but also
sensitive to the training initialization, ensuring a balanced training of the
two networks involved is difficult, and tuning their hyperparameters is often
challenging or unfeasible~\cite{gonog2019review}. 

As discussed in Section~\ref{sec:introduction}, research on resampling methods
with mixed data types is scarce. The original paper proposing SMOTE also
proposed SMOTE for Nominal and Continuous (SMOTENC), an adaptation of SMOTE
handle datasets with nominal and continuous features~\cite{Chawla2002}. To
determine the k-nearest neighbors of $x^c$, the distance is calculated by
incorporating into the Euclidean distance the median of the standard
deviations of the continuous features for every categorical feature with
different values. Once $x^c$ and $x^{nn}$ have been determined, the values of
the continuous features in $x^{gen}$ are generated using the SMOTE generation
mechanism, while the categorical features are given the most common values
occurring in the k-nearest neighbors.

Alternatively to SMOTE-based methods, some non-informed over and undersampling
methods may also be used for datasets with nominal and continuous features,
specifically Random Oversampling (ROS) and Random Undersampling (RUS). These
methods consist in randomly duplicating minority class observations (in the
case of ROS), which can lead to overfitting~\cite{park2021combined,
batista2004study}, or randomly removing majority class observations (in the
case of RUS), which may lead to underfitting~\cite{bansal2021analysis}.

Only two oversampling algorithms capable of handling nominal and continuous
features were found. Recently, a new SMOTE-based oversampling method for
datasets with mixed data types, SMOTE-ENC~\cite{mukherjee2021smote}, was
proposed. This method modifies the encoding mechanism for categorical features
used in the SMOTENC algorithm to account for categorical features' change of
association with minority classes. The Multivariate Normal Distribution-based
Oversampling for Numerical and Categorical features
(MNDO-NC)~\cite{ambai2019multivariate} uses the original MNDO
method~\cite{ambai2018mndo} along with the SMOTENC encoding mechanism to find
the values of the categorical features for the synthetic observation. However,
the results reported in the paper showed that MNDO-NC was consistently
outperformed by SMOTENC, which led us to discard this approach from further
consideration.

\section{Motivation}~\label{sec:motivation}

- There is no method available in the literature that addresses mixed data
  types while modifying the generation mechanism

- SMOTENC increases the possibility of near duplicates. This phenomenon was
  already visible with the original SMOTE oversampler. The generation of
  synthetic observations containing categorical features populated by
  majority voting further amplifies this effect. Therefore, increasing the
  diversity of the synthetic data generated becomes particularly relevant.

- The selection mechanism of Geometric-SMOTE has shown effective results in
  reducing the generation of noisy observations. This mechanism is not
  present in any of the relevant oversamplers discussed.


\section{Proposed Method}~\label{sec:proposed_method}

We propose G-SMOTENC to handle both nominal and continuous features.  This an
extension of the original G-SMOTE oversampler, which affects both its
selection and generation mechanisms. Due to the novelty of the work, these
modifications are based on the SMOTENC mechanism. However, this method can be
extended with further modifications to the categorical data encoding and
selection mechanisms in future work. The G-SMOTENC algorithm aims to address
the limitations described in Section~\ref{sec:motivation}.

Similarly to G-SMOTE being an extension of SMOTE, G-SMOTENC is also an
extension of SMOTENC since any method or ML pipeline using the SMOTENC
generation mechanism can replace it by G-SMOTENC without any further
modifications. The proposed method is described in pseudo-code in
Algorithm~\ref{alg:gsmotenc}. The functions $SelectionMechanism$ and
$GenerationMechanism$ are described in Algorithms~\ref{alg:selection}
and~\ref{alg:generation}, respectively.

\begin{algorithm}
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwInput{KwGiven}{Given}
    \caption{G-SMOTENC.}\label{alg:gsmotenc}
    \DontPrintSemicolon%
    
    % START
    \KwGiven{Dataset with binary target classes $C_{min}$ and $C_{maj}$}
    \KwIn{$C_{maj}, C_{min}, \alpha_{sel}, \alpha_{trunc}, \alpha_{def}$}
    \KwOut{$C^{gen}$}

    \Begin{
        $N \leftarrow |C_{maj}| - |C_{min}|$ \\
        $C^{gen} \leftarrow \emptyset$\\
        \While{$|C^{gen}| < N$}{
            $x^c, x^{nn}, X^{nn} \leftarrow SelectionMechanism(C_{maj},
            C_{min}, \alpha_{sel})$\\
            $x^{gen} \leftarrow GenerationMechanism(x^c, x^{nn}, X^{nn},
            \alpha_{trunc}, \alpha_{def})$\\
            $C^{gen} \leftarrow C^{gen} \cup \{x^{gen}\}$
        }
    }
\end{algorithm}

\subsection{Selection Mechanism}

The data selection mechanism is preceded by the numerical encoding of the
categorical features. It mixes the selection mechanisms of SMOTENC and
G-SMOTENC, as shown in Algorithm~\ref{alg:selection}.

\begin{algorithm}[ht]
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwInput{KwGiven}{Given}
    \caption{G-SMOTENC's selection mechanism.}\label{alg:selection}
    \DontPrintSemicolon%
    
    % START
    \KwIn{$C_{maj}, C_{min}, \alpha_{sel}$}
    \KwOut{$x^c, x^{nn}$, $X^{nn}$}

    \Fn{CatEncoder($C_{maj}$, $C_{min}$)}{
        $S \leftarrow $ Standard deviations of the continuous features in $C_{min}$\\
        $\sigma_{med} \leftarrow median(S)$\\
        \ForAll{$i \in \{maj, min\}$}{
            \ForAll{$f \in C_i^T$}{
                \If{f is categorical}{
                    $f' \leftarrow OneHotEncode(f) \times \sigma_{med} / 2$ \\
                    $C_i' \leftarrow (C_i^T \setminus f)^T$\\
                    $C_i' \leftarrow (C_i'^T \cup f')^T$
                }
            }
        }
        \Return $C_{maj}'$, $C_{min}'$
    }

    \Fn{Surface($\alpha_{sel}$, $x^c$, $C_{maj}$, $C_{min}$)}{
        \If{$\alpha_{sel} = minority$}{
            $x^{nn} \in C_{min, k}$ 
            \tcp*[f]{One of the $k$-nearest neighbors of $x^c$ from
            $C_{min}$}\\
            $X^{nn} \leftarrow C_{min, k}$
        }
        \If{$\alpha_{sel} = majority$}{
            $x^{nn} \in C_{maj, 1}$ 
            \tcp*[f]{Nearest neighbor of $x^c$ from $C_{min}$}\\
            $X^{nn} \leftarrow C_{maj, 1}$
        }
        \If{$\alpha_{sel}$ = combined}{
            $x^{nn}_{min} \in C_{min, k}$ \\
            $x^{nn}_{maj} \in C_{maj, 1}$ \\
            $x^{nn} \leftarrow argmin(||x^{nn}_{min}-x^c||,
            ||x^{nn}_{maj}-x^c||)$\\
            $X^{nn} \leftarrow C_{min, k} \cup C_{maj, 1}$
        }
        \Return $x^{nn}, X^{nn}$ \tcp*[f]{$X^{nn}$ is the set of $k$-nearest neighbors}\\
    }

    \Begin{
        $C_{maj}', C_{min}' \leftarrow CatEncoder(C_{maj}, C_{min})$ \\
        $x^c \in C_{min}'$ \tcp*[f]{Randomly select $x^c$ from $C_{min}'$}\\
        $x^{nn}, X^{nn} \leftarrow Surface(\alpha_{sel}, x^c, C_{maj}', C_{min}')$\\
        Reverse encoding of nominal features in $x^c$, $x^{nn}$ and $X^{nn}$
    }
\end{algorithm}

The selection mechanism uses the minority, majority and combined mechanisms
(introduced by G-SMOTE). However, the nominal features in the minority and
majority class observations, $C_{maj}$ and $C_{min}$ are first encoded using a
one-hot encoding approach and replacing the constant 1 with the median of the
standard deviations of the continuous features in $C_{min}$ divided by 2. The
nearest-neighbors ($X^{nn}$) of $x^c$ are determined based on $\alpha_{sel}$,
which are passed on to the generation mechanism to determine the nominal
features' values of $x^{gen}$ in the generation mechanism. Simultaneously,
$x^{nn}$ is randomly selected from $X^{nn}$ and will be used to generate
$x^{gen}$'s continuous features' values.

\subsection{Generation Mechanism}

G-SMOTENC's generation mechanism is shown in Algorithm~\ref{alg:generation}.
It divides the generation of $x^{gen}$ into two parts: (1) generation of
continuous feature values and (2) generation of nominal feature values. At
this stage, the nominal features from $x^c$ and $x^{nn}$ are discarded.
Afterwards, the continuous features are generated using G-SMOTE's generation
mechanism; within a hypersphere formed using the truncation and deformation
hyperparameters ($\alpha_{trunc}$ and $\alpha_{def}$, respectively).  Finally,
the nominal feature values are generated by the mode of each feature within
the observations in $X^{nn}$.

\begin{algorithm}
    \SetKwInput{KwGiven}{Given}
    \SetKwProg{Fn}{Function}{:}{end}
    \caption{G-SMOTENC's generation mechanism.}\label{alg:generation}
    \DontPrintSemicolon%

    % START
    \KwIn{$x^c, x^{nn}, X^{nn}, \alpha_{trunc}, \alpha_{def}$}
    \KwOut{$x^{gen}$}

    \Fn{Hyperball()}{
        $v_i \sim \mathcal{N}(0, 1)$\\
        $r \sim \mathcal{U}(0, 1)$\\
        $x^{gen} \leftarrow r^{1/p}\frac{(v_1, \ldots, v_p)}{||(v_1, \ldots,
        v_p)||}$\\
        \Return $x^{gen}$
    }

    \Fn{Vectors($x^c, x^{nn}, x^{gen}$)}{
        $e^{//} \leftarrow \frac{x^{nn}-x^c}{||x^{nn}-x^c||}$\\
        $x^{//} \leftarrow (x^{gen} \cdot e^{//})e^{//}$\\
        $x^{\perp} \leftarrow x^{gen} - x^{//}$\\
        \Return $x^{//}, x^{\perp}$
    }

    \Fn{Truncate($x^c, x^{nn}, x^{gen}, x^{//}, \alpha_{trunc}$)}{
        \If{$|\alpha_{trunc} - x^{//}| > 1$}{
            $x^{gen} \leftarrow x^{gen} - 2x^{//}$
        }
        \Return $x^{gen}$
    }

    \Fn{Deform($x^{gen}, x^{\perp}, \alpha_{def}$)}{
        \Return $x^{gen} - \alpha_{def}x^{\perp}$
    }

    \Fn{Translate($x^c, x^{gen}, R$)}{
        \Return $x^c + R x^{gen}$
    }

    \Fn{GenNominal($X^{nn}$)}{
        $x^{gen}_{nom} = \emptyset $\\
        \ForAll{$f \in (X^{nn})^T$}{
            \If{f is categorical}{
                $x^{gen}_{nom} \cup \{mode(f)\}$\tcp*[f]{Ties are decided with
                random selection}\\
            }
        }
        \Return $x^{gen}_{nom}$
    }

    \Begin{
        Discard nominal features from $x^c$ and $x^{nn}$\\
        $x^{gen} \leftarrow Hyperball()$\\
        $x^{//}, x^{\perp} \leftarrow Vectors(x^c, x^{nn}, x^{gen})$\\
        $x^{gen} \leftarrow Truncate(x^c, x^{nn}, x^{gen}, x^{//}, \alpha_{trunc})$\\
        $x^{gen} \leftarrow Deform(x^{gen}, x^{\perp}, \alpha_{def})$\\
        $x^{gen} \leftarrow Translate(x^c, x^{gen},
        ||x^{nn}_{cont}-x^c||)$\\
        $x^{gen}_{nom} \leftarrow GenNominal(X^{nn})$\\
        $x^{gen} \leftarrow x^{gen} \cup x^{gen}_{nom}$
    }

\end{algorithm}

G-SMOTENC contains 3 hyperparameters: the selection strategy ($\alpha_{sel}$),
the truncation factor ($\alpha_{trunc}$) and the deformation factor
($\alpha_{def}$). Figure~\ref{fig:gsmote} depicts the effect of those
hyperparameters in the data selection and generation phases. For an in-depth
definition of the hyperparameters mentioned the reader may follow reference
\cite{douzas2019geometric}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{../analysis/g-smote}
    \caption{A visual depiction of G-SMOTENC. In this example,
        $\alpha_{trunc}$ is approximately 0.5 and $\alpha_{def}$ is
        approximately 0.4.
    }~\label{fig:gsmote}
\end{figure}

\section{Methodology}~\label{sec:methodology}

This section describes how the evaluation of G-SMOTENC was performed. We
describe the datasets used in the experiment, their source and preprocessing
steps carried out in Section~\ref{sec:experimental_data}. We describe the
resampling and classifications methods used for comparing the performance of
G-SMOTENC with other relevant oversampling and undersampling mthods in
Section~\ref{sec:ml_algorithms}. The performance metrics used are defined in
Section~\ref{sec:performance_metrics}. Finally, the experimental procedure is
described in Section~\ref{sec:experimental_procedure}.

\subsection{Experimental Data}~\label{sec:experimental_data}

The datasets used in this experiment were extracted from the
\href{https://archive.ics.uci.edu}{UC Irvine Machine Learning Repository}. All
of the datasets are publicly available and cover a range of different domains.
The selection of datasets was done to ensure that all datasets are imbalanced
and contained non-metric features (\textit{i.e.}, whether ordinal, nominal or
binary). These datasets will be used to show how the performance of different
classifiers varies according to the used over/undersampling method.

At an initial stage, all datasets were preprocessed manually with minimal
manipulations, to avoid the application of preprocessing methods beyond the
scope of this paper. This step was conducted to remove features and/or
observations with missing values and identifying the non-metric features. The
second stage of our preprocessing was done systematically. The resulting
datasets are shown in Table~\ref{tbl:datasets_description}.

\input{../analysis/datasets_description.tex}

The second part of the data preprocessing pipeline starts with the generation
of artificially imbalanced datasets with different Imbalance Ratios
($IR=\frac{|C_{maj}|}{|C_{min}|}$). For each original dataset, we create its
more imbalanced versions at intervals of 10, while ensuring that $|C_{min}|
\ge 15$. The sampling strategy was determined for class $n \in
\{1,\ldots,n,\ldots,m\}$ as a linear interpolation using $|C_{maj}|$ and
$|C_{min}'|=\frac{|C_{maj}|}{IR_{new}}$, as shown in
equation~\ref{eq:sampling}.

\begin{equation}~\label{eq:sampling}
    |C_i|^{imb} =
    \min(\frac{|C_{min}'|-|C_{maj}|}{n-1}.|C_i|+|C_{max}|, |C_i|)
\end{equation}

The new, artificially imbalanced dataset, is formed by sampling observations
without replacement from each $C_i$ such that $C_i' \subseteq C_i , |C_i'| =
|C_i|^{imb}$. The artificially imbalanced datasets are marked with its
imbalance ratio as a suffix in Table~\ref{tbl:datasets_description}.

The datasets (both original and artificially imbalanced versions) are then
filtered to ensure all datasets have a minimum of 500 observations.  The
remaining datasets whose number of observations is larger than 5000 are
randomly sampled to match this number of observations. Afterwards, for each
remaining dataset we remove all observations from target classes whose
frequency is lower than 15 observations. Finally, the continuous and discrete
features are scaled to the range $[0,1]$ to ensure a common range between all
features. 

\subsection{Machine Learning Algorithms}~\label{sec:ml_algorithms}

The choice of classifiers used in the experimental procedure were based on
their type (tree-based, nearest neighbors-based, linear model and
ensemble-based), popularity and consistency in performance. We used Decision
Tree (DT), a K-Nearest Neighbors (KNN) classifier, a Logistic
Regression (LR) and a Random Forest (RF).

Given the lack of existing oversamplers that address imbalanced learning
problems with mixed data types, the amount of benchmark methods used is also
limited. We used the well known methods and one state-of-the-art oversampling
method that are compatible with this type of datasets: SMOTENC, RUS, ROS and
SMOTE-ENC.  Table~\ref{tbl:grid} shows the hyperparameters used for the
parameter search described in Section~\ref{sec:experimental_procedure}.

\begin{table}
	\centering
    \caption{\label{tbl:grid}
        Hyperparameter definition for the classifiers and resamplers used in
        the experiment.
    }
	\begin{tabular}{lll}
		\toprule
		Classifier      & Parameter                        & Values                         \\
		\midrule
        DT              & min.\ samples split              & 2                              \\
                        & criterion                        & gini                           \\
                        & max depth                        & 3, 6                           \\
		LR              & maximum iterations               & 10000                          \\
                        & multi-class                      & One-vs-All                     \\
		                & solver                           & saga                           \\
                        & penalty                          & None, L1, L2                   \\
		KNN             & \# neighbors                     & 3, 5                           \\
                        & weights                          & uniform                        \\
                        & metric                           & euclidean                      \\
		RF              & min.\ samples split              & 2                              \\
		                & \# estimators                    & 50, 100                        \\
		                & Max depth                        & 3, 6                           \\
                        & criterion                        & gini                           \\
		\toprule
		Resampler       & Parameter                        & Values                         \\
		\midrule
		SMOTENC         & \# neighbors                     & 3, 5                           \\
		SMOTE-ENC       & \# neighbors                     & 3, 5                           \\
		G-SMOTENC       & \# neighbors                     & 3, 5                           \\
                        & deformation factor               & 0.0, 0.25, 0.5, 0.75, 1.0      \\
                        & truncation factor                & -1.0, -0.5, 0.0, 0.5, 1.0      \\
                        & selection strategy               & ``combined'',
                        ``minority'', ``majority''\\
		RUS             & replacement                      & False                          \\
		ROS             & (no applicable parameters)       &                                \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Performance Metrics}~\label{sec:performance_metrics}

The choice of the performance metric plays a critical role in the assessment
of effect on classification tasks. The typical performance metrics,
\textit{e.g.}, Overall Accuracy (OA), are intuitive to interpret but are often
inappropriate to measure a classifier's performance in an imbalanced learning
context~\cite{sun2009classification}. For example, to estimate an event that
occurs in 1\% of the dataset, a constant classifier would obtain an OA of 0.99
and still be unusable. However, this metric is still reported in some of our
results to maintain a metric that is easier to interpret.

More recent surveys have found the Geometric-mean ($\textit{G-mean} =
\sqrt{\overline{Sensitivity} \times \overline{Specificity}}$), F1-score
($\textit{F-score}=2\times\frac{\overline{Precision} \times
\overline{Recall}}{\overline{Precision} + \overline{Recall}}$), $Sensitivity =
\frac{TP}{FN+TP}$ and $Specificity = \frac{TN}{TN + FP}$ to be commonly used
performance metrics in imbalanced learning contexts~\cite{rout2018handling}.
These metrics are calculated as a function of the number of False/True
Positives (FP and TP) and False/True Negatives (FN and TN), having
$Precision = \frac{TP}{TP+FP}$ and $Recall = \frac{TP}{TP+FN}$.
This finding is consistent with other well-known recommendations on the usage
of performance metrics~\cite{jeni2013facing}. This led us to adopt, along with
OA, both F-score and G-mean as the main performance metrics for this study. 

\subsection{Experimental Procedure}~\label{sec:experimental_procedure}

The experimental procedure was applied similarly to all combinations of
resamplers, classifiers and hyperparameter combinations across all datasets.
The evaluation of the models' performance was tested using a 5-fold Cross
Validation (CV) approach. The mean performance in the test set is calculated
over the 5 folds and 3 different runs of the experimental procedure for each
combination resampling/classifier hyperparameters. For each dataset, results
of the hyperparameters that optimize the performance of a resampler/classifier
are selected. These results were then used for analysis and are shown in
Table~\ref{tbl:wide_optimal} (see Appendix).
Figure~\ref{fig:experimental_procedure} shows a diagram of the experimental
procedure described.

\begin{figure}
	\centering
	\includegraphics[width=.8\linewidth]{../analysis/experimental_procedure}
    \caption{Experimental procedure used in this study.
    }~\label{fig:experimental_procedure}
\end{figure}

A CV run consists of a stratified partitioning (\textit{i.e.}, each partition
contains the same relative frequencies of target labels) of the dataset into
five parts. A given resampler/classifier combination with a specific set of
hyperparameters is fit and tested five times, using one of the partitions as a
test set and the remaining ones as training set. The estimated performance
consists of the average classification performance across the five different
test sets. 

\subsection{Software Implementation}~\label{sec:software_implementation}

The algorithmic implementation of G-SMOTENC was written using the Python
programming language and is available in the open-source package
\href{https://github.com/joaopfonseca/ml-research}{ML-Research}~\cite{fonseca2021increasing},
along with other utilities used to produce the experiment and outputs used in
Section~\ref{sec:results_and_discussion}. In addition, the packages
\href{https://github.com/scikit-learn/scikit-learn/}{Scikit-Learn}~\cite{scikit-learn},
\href{https://github.com/scikit-learn-contrib/imbalanced-learn}{Imbalanced-Learn}~\cite{JMLR:v18:16-365}
and \href{https://github.com/georgedouzas/research-learn/}{Research-Learn}
were also used in the experimental procedure to get the implementations of the
classifiers, benchmark over/undersamplers and run the experimental procedure.
The original SMOTE-ENC implementation was retrieved from the
\href{https://github.com/Mimimkh/SMOTE-ENC-code}{authors' GitHub repository}.
The Latex code, Python scripts (including data pulling and preprocessing,
experiment setup and results' analysis), as well as the datasets used are
available in this \href{https://github.com/joaopfonseca/publications}{GitHub
repository}.
 

\section{Results and Discussion}~\label{sec:results_and_discussion}

In this section we present the experimental results. We focus on the
comparison of classification performance using oversamplers whose generation
mechanism is compatible with datasets containing both continuous and
categorical features.

The analysis of our experimental results were developed in two stages: (1)
analysis of mean ranking and absolute performance and (2) statistical
analysis. In Section~\ref{sec:discussion} we discuss the main insights
extracted by analysing the results reported in Sections~\ref{sec:results}
and~\ref{sec:statistical_analysis}.

\subsection{Results}~\label{sec:results}

Table~\ref{tbl:mean_sem_ranks} presents the mean rankings of cross validation
scores across the different combinations of oversamplers, metrics and
classifiers. These results were calculated by assigning a ranking score for
each oversampler from 1 (best) to 4 (worst) for each dataset, metric and
classifier, based on the results reported in Table~\ref{tbl:wide_optimal} (see
Appendix).

\input{../analysis/mean_sem_ranks.tex}

Table~\ref{tbl:mean_sem_scores} presents the mean cross validation scores.
With exception to the OA metric, G-SMOTENC either outperformed or matched the
the remaining oversamplers.

\input{../analysis/mean_sem_scores.tex}

\subsection{Statistical Analysis}~\label{sec:statistical_analysis}

To conduct an appropriate statistical analysis in an experiment with multiple
datasets, it is necessary to use methods that account for the multiple
comparison problem. Based on the recommendations found in~\cite{Demsar2006},
we applied the Friedman test along with the Holm-Bonferroni test for a
post-hoc analysis.

In Section~\ref{sec:performance_metrics} we
explained that OA, although easily interpretable, is not an appropriate
performance metric for imbalanced learning problems. Therefore, the
statistical analysis was developed using the two imbalance-appropriate metrics
used in the study: F-Score and G-Mean. The statistical analysis started with
the assessment of a statistically significant difference in performance across
resampling methods using a Friedman test~\cite{friedman1937use}. The results
of this test are shown in Table~\ref{tbl:friedman_test}. The null hypothesis
is rejected in all cases.

\input{../analysis/friedman_test.tex}

We performed a Holm-Bonferroni test to understand whether the difference in
performance of G-SMOTENC is statistically significant to the remaining
resampling methods. The results of this test are shown in
Table~\ref{tbl:holms_test}. The null hypothesis was rejected in 27 out of 32 tests.

\input{../analysis/holms_test.tex}

\subsection{Discussion}~\label{sec:discussion}

The results reported in Section~\ref{sec:results} show that\ldots

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{../analysis/consistency_analysis_plot}
    \caption{Average ranking of oversamplers over different characteristics of
        the datasets used in the experiment. Legend: IR --- Imbalance Ratio,
        Classes --- Number of classes in the dataset, M/NM ratio --- ratio
        between the number of metric and non-metric features, E$($F-Score$)$
        --- Mean F-Score of dataset across all combinations of classifiers and
        oversamplers.
    }~\label{fig:consistency_analysis}
\end{figure}


The results from this experiment expose some well-known limitations of SMOTE
which become particularly evident with SMOTENC. Specifically, the loss of data
variability and in some occasions the near-duplication of observations reveal
that the performance of SMOTENC is comparable to ROS' performance.

\section{Conclusion}~\label{sec:conclusion}

This is a conclusion.

Future work should focus on more sophisticated encoding mechanisms for the
categorical features.

\bibliography{references}
\bibliographystyle{ieeetr}

\appendix

\section{Appendix}

\input{../analysis/wide_optimal.tex}

\end{document}
