@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}

@misc{ introForbes,
	title = "500,000 Families May Be Homeless Due To Devastation From Cyclone Amphan",
	howpublished = {\url{https://www.forbes.com/sites/tommybeer/2020/05/22/500000-families-may-be-homeless-due-to-devastation-from-cyclone-amphan/}},
	titleaddon = "Forbes",
	author = "Tommy Beer",
	urldate = "2020-09-01",
	note = "Section: Business",
	year = {2020}
}

@misc{ introNasa,
	title = "Amphan Batters India, Bangladesh",
	howpublished = {\url{https://earthobservatory.nasa.gov/images/146749/amphan-batters-india-bangladesh}},
	author = "Kathryn Hansen",
	urldate = {2020-09-01},
	month = "May",
	day = "20",
	year = "2020",
	publisher = "NASA Earth Observatory"
}

@misc{ introIwmi,
	title = "Cyclone Amphan and Covid-19: the recipe for a cascading disaster",
	howpublished = {\url{http://www.iwmi.cgiar.org/2020/06/cyclone-amphan-and-covid-19-the-recipe-for-a-cascading-disaster/}},
	author = "Aditi Mukherji",
    month = "June",
	day = "10",
	year = "2020",
	urldate = "2020-09-01"
}

@misc{ introRedcross,
	title = "Operation Update Report India: Cyclone Amphan",
	howpublished = {\url{https://reliefweb.int/report/india/india-cyclone-amphan-operation-update-report-dref-n-mdrin025}},
	author = "International Federation of Red Cross and Red Crescent Societies",
	month = "July",
	day = "23",
	year = "2020",
	urldate = {2020-09-02}
}

@misc{ introState,
	title = "Joint Rapid Need Assessment Report on Cyclone Amphan",
	author = "State Inter Agency Group of West Bengal",
	month = "June",
	year = "2020"
}

@misc{ conclIAMAI,
	title = "India Internet 2019",
	author = "Internet and Mobile Association of India",
	year = "2019",
	url={https://cms.iamai.in/Content/ResearchPapers/d3654bcc-002f-4fc7-ab39-e1fbeb00005d.pdf},
	urldate = {2020-09-09}
}

@inproceedings{Hallac2019,
    author = {Hallac, Ibrahim R. and Makinist, Semiha and Ay, Betul and Aydin, Galip},
    booktitle = {2019 International Conference on Artificial Intelligence and Data Processing Symposium, IDAP 2019},
    doi = {10.1109/IDAP.2019.8875952},
    isbn = {9781728129327},
    keywords = {Twitter mining,user embeddings},
    month = {sep},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    title = {{User2Vec: Social Media User Representation Based on Distributed     Document Embeddings}},
    year = {2019}
}

@misc{lewis2019bart,
    title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
    author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
    year={2019},
    eprint={1910.13461},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{yin2019benchmarking,
    title={Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach},
    author={Wenpeng Yin and Jamaal Hay and Dan Roth},
    year={2019},
    eprint={1909.00161},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Hutto2014,
abstract = {The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a goldstandard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.},
author = {Hutto, C. J. and Gilbert, Eric},
booktitle = {Proceedings of the 8th International Conference on Weblogs and Social Media, ICWSM 2014},
isbn = {9781577356578},
title = {{VADER: A parsimonious rule-based model for sentiment analysis of social media text}},
year = {2014}
}

@inproceedings{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: They lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of- words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc and Mikolov, Tomas},
booktitle = {31st International Conference on Machine Learning, ICML 2014},
eprint = {1405.4053},
isbn = {9781634393973},
title = {{Distributed representations of sentences and documents}},
year = {2014}
}

@article{Dutta2018,
abstract = {We investigate whether off-the-shelf summarization algorithms can be combined to produce better quality summaries. To this end, we propose ensemble schemes that can combine the outputs of multiple base summarization algorithms, to produce summaries better than what is generated by any of the base algorithms.},
author = {Dutta, Soumi and Chandra, Vibhash and Mehra, Kanav and Das, Asit Kumar and Chakraborty, Tanmoy and Ghosh, Saptarshi},
doi = {10.1109/MIS.2018.033001411},
issn = {19411294},
journal = {IEEE Intelligent Systems},
keywords = {ensemble summarization,graph-based summarization,learning to rank,microblog summarization},
title = {{Ensemble Algorithms for Microblog Summarization}},
year = {2018}
}
@inproceedings{Mehra2017,
abstract = {In recent years, Online Social Media (OSM) has established itself as one of the most significant sources of situational information during any natural or man-made disaster. Real-time summarization of this rapidly posted huge volume of crowdsourced responses is a common requirement for emergency relief and preparedness when time is critical. Our semi-automatic method exploits a combination of SumBasic Summarizer and different classifiers to summarize the topic wise relevant microblogs (tweets) extracted through manually identified query term matching. The result of our participation in the SMERP 2017 Data Challenge Track shows that it is an effective approach in summarizing tweets in a disaster scenario and can be replicated across diverse domains.},
author = {Mehra, Kanav and Chandra, Vibhash},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
keywords = {Emergency Relief,Microblog Retrieval,Summarization},
title = {{Summarizing microblogs for emergency relief and preparedness}},
year = {2017}
}
@article{Imran2015,
abstract = {Social media platforms provide active communication channels during mass convergence and emergency events such as disasters caused by natural hazards. As a result, first responders, decision makers, and the public can use this information to gain insight into the situation as it unfolds. In particular, many social media messages communicated during emergencies convey timely, actionable information. Processing social mediamessages to obtain such information, however, involves solving multiple challenges including: parsing brief and informal messages, handling information overload, and prioritizing different types of information found in messages. These challenges can be mapped to classical information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. We survey the state of the art regarding computational methods to process social media messages and highlight both their contributions and shortcomings. In addition, we examine their particularities, and methodically examine a series of key subproblems ranging from the detection of events to the creation of actionable and useful summaries. Research thus far has, to a large extent, produced methods to extract situational awareness information from social media. In this survey, we cover these various approaches, and highlight their benefits and shortcomings.We conclude with research challenges that go beyond situational awareness, and begin to look at supporting decision making and coordinating emergency-response actions.},
author = {Imran, Muhammad and Castillo, Carlos and Diaz, Fernando and Vieweg, Sarah},
doi = {10.1145/2771588},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Crisis computing,Disastermanagement,Mass emergencies,Socialmedia},
title = {{Processing social media messages in Mass Emergency: A survey}},
year = {2015}
}
@inproceedings{Alam2018,
abstract = {During time-critical situations such as natural disasters, rapid classification of data posted on social networks by affected people is useful for humanitarian organizations to gain situational awareness and to plan response efforts. However, the scarcity of labeled data in the early hours of a crisis hinders machine learning tasks thus delays crisis response. In this work, we propose to use an inductive semi-supervised technique to utilize unlabeled data, which is often abundant at the onset of a crisis event, along with fewer labeled data. Specifically, we adopt a graph-based deep learning framework to learn an inductive semi-supervised model. We use two real-world crisis datasets from Twitter to evaluate the proposed approach. Our results show significant improvements using unlabeled data as compared to only using labeled data.},
archivePrefix = {arXiv},
arxivId = {1805.06289},
author = {Alam, Firoj and Joty, Shafiq and Imran, Muhammad},
booktitle = {12th International AAAI Conference on Web and Social Media, ICWSM 2018},
eprint = {1805.06289},
isbn = {9781577357988},
title = {{Graph based semi-supervised learning with convolution neural networks to classify crisis related tweets}},
year = {2018}
}
@article{Hernandez-Suarez2019,
abstract = {In recent years, Online Social Networks (OSNs) have received a great deal of attention for their potential use in the spatial and temporal modeling of events owing to the information that can be extracted from these platforms. Within this context, one of the most latent applications is the monitoring of natural disasters. Vital information posted by OSN users can contribute to relief efforts during and after a catastrophe. Although it is possible to retrieve data from OSNs using embedded geographic information provided by GPS systems, this feature is disabled by default in most cases. An alternative solution is to geoparse specific locations using language models based on Named Entity Recognition (NER) techniques. In this work, a sensor that uses Twitter is proposed to monitor natural disasters. The approach is intended to sense data by detecting toponyms (named places written within the text) in tweets with event-related information, e.g., a collapsed building on a specific avenue or the location at which a person was last seen. The proposed approach is carried out by transforming tokenized tweets into word embeddings: a rich linguistic and contextual vector representation of textual corpora. Pre-labeled word embeddings are employed to train a Recurrent Neural Network variant, known as a Bidirectional Long Short-Term Memory (biLSTM) network, that is capable of dealing with sequential data by analyzing information in both directions of a word (past and future entries). Moreover, a Conditional Random Field (CRF) output layer, which aims to maximize the transition from one NER tag to another, is used to increase the classification accuracy. The resulting labeled words are joined to coherently form a toponym, which is geocoded and scored by a Kernel Density Estimation function. At the end of the process, the scored data are presented graphically to depict areas in which the majority of tweets reporting topics related to a natural disaster are concentrated. A case study on Mexico's 2017 Earthquake is presented, and the data extracted during and after the event are reported.},
author = {Hernandez-Suarez, Aldo and Sanchez-Perez, Gabriel and Toscano-Medina, Karina and Perez-Meana, Hector and Portillo-Portillo, Jose and Sanchez, Victor and Villalba, Luis Javier Garc{\'{i}}a},
doi = {10.3390/s19071746},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {CRF,Data mining,Geocoding,Geoparsing,LSTM,Twitter,Word2vec},
pmid = {30979067},
title = {{Using twitter data to monitor natural disaster social dynamics: A recurrent neural network approach with word embeddings and kernel density estimation}},
year = {2019}
}
@inproceedings{Imran2014,
abstract = {We present AIDR (Artificial Intelligence for Disaster Re- sponse), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply hu- man intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that peo- ple post during disasters into a set of user-defined categories of information (e.g., $\backslash$needs", $\backslash$damage", etc.) For this pur- pose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification tech- niques) and leverages human-participation (through crowd- sourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted dur- ing the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80{\%}. AIDR is available at http://aidr.qcri.org/.},
author = {Imran, Muhammad and Castillo, Carlos and Lucas, Ji and Meier, Patrick and Vieweg, Sarah},
booktitle = {WWW 2014 Companion - Proceedings of the 23rd International Conference on World Wide Web},
doi = {10.1145/2567948.2577034},
isbn = {9781450327459},
keywords = {Classification,Crowdsourcing,Online machine learning,Stream processing},
title = {{AIDR: Artificial intelligence for disaster response}},
year = {2014}
}
@article{Zhang2016,
abstract = {The first objective towards the effective use of microblogging services such as Twitter for situational awareness during the emerging disasters is discovery of the disaster-related postings. Given the wide range of possible disasters, using a pre-selected set of disaster-related keywords for the discovery is suboptimal. An alternative that we focus on in this work is to train a classifier using a small set of labeled postings that are becoming available as a disaster is emerging. Our hypothesis is that utilizing large quantities of historical microblogs could improve the quality of classification, as compared to training a classifier only on the labeled data. We propose to use unlabeled microblogs to cluster words into a limited number of clusters and use the word clusters as features for classification. To evaluate the proposed semi-supervised approach, we used Twitter data from 6 different disasters. Our results indicate that when the number of labeled tweets is 100 or less, the proposed approach is superior to the standard classification based on the bag or words feature representation. Our results also reveal that the choice of the unlabeled corpus, the choice of word clustering algorithm, and the choice of hyperparameters can have a significant impact on the classification accuracy.},
archivePrefix = {arXiv},
arxivId = {1610.03750},
author = {Zhang, Shanshan and Vucetic, Slobodan},
eprint = {1610.03750},
keywords = {crisis management,semi-supervised learning,twitter},
title = {{Semi-supervised Discovery of Informative Tweets During the Emerging Disasters}},
url = {http://arxiv.org/abs/1610.03750},
year = {2016}
}
@article{To2017,
abstract = {Social media such as tweets are emerging as platforms contributing to situational awareness during disasters. Information shared on Twitter by both affected population (e.g., requesting assistance, warning) and those outside the impact zone (e.g., providing assistance) would help first responders, decision makers, and the public to understand the situation first-hand. Effective use of such information requires timely selection and analysis of tweets that are relevant to a particular disaster. Even though abundant tweets are promising as a data source, it is challenging to automatically identify relevant messages since tweet are short and unstructured, resulting to unsatisfactory classification performance of conventional learning-based approaches. Thus, we propose a simple yet effective algorithm to identify relevant messages based on matching keywords and hashtags, and provide a comparison between matching-based and learning-based approaches. To evaluate the two approaches, we put them into a framework specifically proposed for analyzing diaster-related tweets. Analysis results on eleven datasets with various disaster types show that our technique provides relevant tweets of higher quality and more interpretable results of sentiment analysis tasks when compared to learning approach.},
archivePrefix = {arXiv},
arxivId = {1705.02009},
author = {To, Hien and Agrawal, Sumeet and Kim, Seon Ho and Shahabi, Cyrus},
doi = {10.1109/BigMM.2017.82},
eprint = {1705.02009},
isbn = {9781509065493},
journal = {Proceedings - 2017 IEEE 3rd International Conference on Multimedia Big Data, BigMM 2017},
keywords = {Emergency Management,Social Media},
pages = {330--337},
title = {{On Identifying Disaster-Related Tweets: Matching-Based or Learning-Based}},
year = {2017}
}
@inproceedings{Imran2013,
abstract = {During times of disasters online users generate a significant amount of data, some of which are extremely valuable for relief efforts. In this paper, we study the nature of social-media content generated during two different natural disasters. We also train a model based on conditional random fields to extract valuable information from such content. We evaluate our techniques over our two datasets through a set of carefully designed experiments. We also test our methods over a non-disaster dataset to show that our extraction model is useful for extracting information from socially-generated content in general.},
author = {Imran, Muhammad and Elbassuoni, Shady and Castillo, Carlos and Diaz, Fernando and Meier, Patrick},
booktitle = {WWW 2013 Companion - Proceedings of the 22nd International Conference on World Wide Web},
doi = {10.1145/2487788.2488109},
isbn = {9781450320382},
keywords = {Information extraction,Information filtering,Social media},
title = {{Practical extraction of disaster-relevant information from social media}},
year = {2013}
}
@inproceedings{Imran2013a,
abstract = {Microblogging sites such as Twitter can play a vital role in spreading information during "natural" or man-made disasters. But the volume and velocity of tweets posted during crises today tend to be extremely high, making it hard for disaster-affected communities and professional emergency responders to process the information in a timely manner. Furthermore, posts tend to vary highly in terms of their subjects and usefulness; from messages that are entirely off-topic or personal in nature, to messages containing critical information that augments situational awareness. Finding actionable information can accelerate disaster response and alleviate both property and human losses. In this paper, we describe automatic methods for extracting information from microblog posts. Specifically, we focus on extracting valuable "information nuggets", brief, self-contained information items relevant to disaster response. Our methods leverage machine learning methods for classifying posts and information extraction. Our results, validated over one large disaster-related dataset, reveal that a careful design can yield an effective system, paving the way for more sophisticated data analysis and visualization systems.},
author = {Imran, Muhammad and Elbassuoni, Shady and Castillo, Carlos and Diaz, Fernando and Meier, Patrick},
booktitle = {ISCRAM 2013 Conference Proceedings - 10th International Conference on Information Systems for Crisis Response and Management},
isbn = {9783923704804},
keywords = {Information extraction,Social media,Supervised classification,Twitter},
title = {{Extracting information nuggets from disaster- Related messages in social media}},
year = {2013}
}
@article{Arachie2020,
abstract = {Social media plays a major role during and after major natural disasters (e.g., hurricanes, large-scale fires, etc.), as people “on the ground” post useful information on what is actually happening. Given the large amounts of posts, a major challenge is identifying the information that is useful and actionable. Emergency responders are largely interested in finding out what events are taking place so they can properly plan and deploy resources. In this paper we address the problem of automatically identifying important sub-events (within a large-scale emergency “event”, such as a hurricane). In particular, we present a novel, unsupervised learning framework to detect sub-events in Tweets for retrospective crisis analysis. We first extract noun-verb pairs and phrases from raw tweets as sub-event candidates. Then, we learn a semantic embedding of extracted noun-verb pairs and phrases, and rank them against a crisis-specific ontology. We filter out noisy and irrelevant information then cluster the noun-verb pairs and phrases so that the top-ranked ones describe the most important sub-events. Through quantitative experiments on two large crisis data sets (Hurricane Harvey and the 2015 Nepal Earthquake), we demonstrate the effectiveness of our approach over the state-of-the-art. Our qualitative evaluation shows better performance compared to our baseline.},
archivePrefix = {arXiv},
arxivId = {1912.13332},
author = {Arachie, Chidubem and Gaur, Manas and Anzaroot, Sam and Groves, William and Zhang, Ke and Jaimes, Alejandro},
doi = {10.1609/aaai.v34i01.5370},
eprint = {1912.13332},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
number = {01},
pages = {354--361},
title = {{Unsupervised Detection of Sub-Events in Large Scale Disasters}},
volume = {34},
year = {2020}
}
@article{Rudra2018,
abstract = {In recent times, humanitarian organizations increasingly rely on social media to search for information useful for disaster response. These organizations have varying information needs ranging from general situational awareness (i.e., to understand a bigger picture) to focused information needs e.g., about infrastructure damage, urgent needs of affected people. This research proposes a novel approach to help crisis responders fulfill their information needs at different levels of granularities. Specifically, the proposed approach presents simple algorithms to identify sub-events and generate summaries of big volume of messages around those events using an Integer Linear Programming (ILP) technique. Extensive evaluation on a large set of real world Twitter dataset shows (a). our algorithm can identify important sub-events with high recall (b). The summarization scheme shows (6 - -30{\%}) higher accuracy of our system compared to many other state-of-the-art techniques. The simplicity of the algorithms ensures that the entire task is done in real time which is needed for practical deployment of the system.},
author = {Rudra, Koustav and Goyal, Pawan and Ganguly, Niloy and Mitra, Prasenjit and Imran, Muhammad},
doi = {10.1145/3209978.3210030},
isbn = {9781450356572},
journal = {41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2018},
keywords = {Class-based summarization,High-level summarization,Humanitarian classes,Situational information,Sub-event detection},
pages = {265--274},
title = {{Identifying sub-events and summarizing disaster-related information from microblogs}},
year = {2018}
}
@inproceedings{Rudra2015,
abstract = {Microblogging sites like Twitter have become important sources of real-time information during disaster events. A significant amount of valuable situational information is available in these sites; however, this information is immersed among hundreds of thousands of tweets, mostly containing sentiments and opinion of the masses, that are posted during such events. To effectively utilize microblogging sites during disaster events, it is necessary to (i) extract the situational information from among the large amounts of sentiment and opinion, and (ii) summarize the situational information, to help decision-making processes when time is critical. In this paper, we develop a novel framework which first classifies tweets to extract situational information, and then summarizes the information. The proposed framework takes into consideration the typicalities pertaining to disaster events where (i) the same tweet often contains a mixture of situational and non-situational information, and (ii) certain numerical information, such as number of casualties, vary rapidly with time, and thus achieves superior performance compared to state-of-the-art tweet summarization approaches.},
author = {Rudra, Koustav and Ghosh, Subham and Ganguly, Niloy and Goyal, Pawan and Ghosh, Saptarshi},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/2806416.2806485},
isbn = {9781450337946},
keywords = {Classification,Disaster events,Situational information,Summarization,Twitter},
title = {{Extracting situational information from microblogs during disaster events: A classification-summarization approach}},
year = {2015}
}
@inproceedings{Rudra2016,
abstract = {During mass convergence events such as natural disasters, microblogging platforms like Twitter are widely used by affected people to post situational awareness messages. These crisis-related messages disperse among multiple categories like infrastructure damage, information about missing, injured, and dead people etc. The challenge here is to extract important situational updates from these messages, assign them appropriate informational categories, and finally summarize big trove of information in each category. In this paper, we propose a novel framework which first assigns tweets into different situational classes and then summarize those tweets. In the summarization phase, we propose a two stage summarization framework which first extracts a set of important tweets from the whole set of information through an Integer-linear programming (ILP) based optimization technique and then follows a word graph and content word based abstractive summarization technique to produce the final summary. Our method is time and memory efficient and outperforms the baseline in terms of quality, coverage of events, locations et al., effectiveness, and utility in disaster scenarios.},
author = {Rudra, Koustav and Banerjee, Siddhartha and Ganguly, Niloy and Goyal, Pawan and Imran, Muhammad and Mitra, Prasenjit},
booktitle = {HT 2016 - Proceedings of the 27th ACM Conference on Hypertext and Social Media},
doi = {10.1145/2914586.2914600},
isbn = {9781450342476},
keywords = {Classification,Disaster events,Situational information,Summarization,Twitter},
title = {{Summarizing situational tweets in crisis scenario}},
year = {2016}
}
@article{Pourebrahim2019,
abstract = {This study investigates Twitter usage during Hurricane Sandy following the survey of the general population and exploring communication dynamics on Twitter through different modalities. The results suggest that Twitter is a highly valuable source of disaster-related information particularly during the power outage. With a substantial increase in the number of tweets and unique users during the Hurricane Sandy, a large number of posts contained firsthand information about the hurricane showing the intensity of the event in real-time. More specifically, a number of images of damage and flooding were shared on Twitter through which researchers and emergency managers can retrieve valuable information to help identify storm damages and plan relief efforts. The social media analysis revealed the most important information that can be derived from twitter during disasters so that authorities can successfully utilize such data. The findings provide insights into the choice of keywords and sentiments and identifying the influential actors at different stages of disasters. A number of key influencers and their followers from different domains including political, news, weather, and relief organizations participated in Twitter-based discussions related to Hurricane Sandy. The connectivity of the influencers and their followers on Twitter plays a vital role in information sharing and dissemination throughout the hurricane. These connections can provide an effective vehicle for emergency managers towards establishing better bi-directional communication during disasters. However, while government agencies were among the prominent Twitter users during the Hurricane Sandy, they primarily relied on one-way communication rather than engaging with their audiences, a challenge that need to be addressed in future research.},
author = {Pourebrahim, Nastaran and Sultana, Selima and Edwards, John and Gochanour, Amanda and Mohanty, Somya},
doi = {10.1016/j.ijdrr.2019.101176},
issn = {22124209},
journal = {International Journal of Disaster Risk Reduction},
keywords = {Disaster management,Hurricane,Information diffusion,Social media,Social network analysis,Twitter},
number = {May},
pages = {101176},
publisher = {Elsevier Ltd},
title = {{Understanding communication dynamics on Twitter during natural disasters: A case study of Hurricane Sandy}},
url = {https://doi.org/10.1016/j.ijdrr.2019.101176},
volume = {37},
year = {2019}
}
@article{Ragini2018,
abstract = {Big data created by social media and mobile networks provide an exceptional opportunity to mine valuable insights from them. This information is harnessed by business entities to measure the level of customer satisfaction but its application in disaster response is still in its inflection point. Social networks are increasingly used for emergency communications and help related requests. During disaster situations, such emergency requests need to be mined from the pool of big data for providing timely help. Though government organizations and emergency responders work together through their respective national disaster response framework, the sentiment of the affected people during and after the disaster determines the success of the disaster response and recovery process. In this paper, we propose a big data driven approach for disaster response through sentiment analysis. The proposed model collects disaster data from social networks and categorize them according to the needs of the affected people. The categorized disaster data are classified through machine learning algorithm for analyzing the sentiment of the people. Various features like, parts of speech and lexicon are analyzed to identify the best classification strategy for disaster data. The results show that lexicon based approach is suitable for analyzing the needs of the people during disaster. The practical implication of the proposed methodology is the real-time categorization and classification of social media big data for disaster response and recovery. This analysis helps the emergency responders and rescue personnel to develop better strategies for effective information management of the rapidly changing disaster environment.},
author = {Ragini, J. Rexiline and Anand, P. M.Rubesh and Bhaskar, Vidhyacharan},
doi = {10.1016/j.ijinfomgt.2018.05.004},
issn = {02684012},
journal = {International Journal of Information Management},
keywords = {Big data,Disaster management,Natural language processing,Sentiment analysis,Social media analysis,Text classification},
title = {{Big data analytics for disaster response and recovery through sentiment analysis}},
year = {2018}
}

@inproceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@inproceedings{SciPyProceedings_11,
address = {Pasadena, CA USA},
author = {Hagberg, Aric A and Schult, Daniel A and Swart, Pieter J},
booktitle = {Proceedings of the 7th Python in Science Conference},
editor = {Varoquaux, Ga{\"{e}}l and Vaught, Travis and Millman, Jarrod},
pages = {11--15},
title = {{Exploring Network Structure, Dynamics, and Function using NetworkX}},
year = {2008}
}

@article{VanDerMaaten2008,
    author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
    issn = {15324435},
    journal = {Journal of Machine Learning Research},
    keywords = {Dimensionality reduction,Embedding     algorithms,Manifold learning,Multidimensional     scaling,Visualization},
    title = {{Visualizing data using t-SNE}},
    url = {https://www.jmlr.org/papers/v9/vandermaaten08a.html},
    year = {2008}
}
@inproceedings{Loper02nltk:the,
    author = {Edward Loper and Steven Bird},
    title = {NLTK: The Natural Language Toolkit},
    booktitle = {Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics},
    year = {2002}
}