\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Synthetic data generation: A literature review}

\author{%
	Joao Fonseca\(^{1*}\), Fernando Bacao\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de
    Campolide, 1070--312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{%
	a4paper,
	left=18mm,
	right=18mm,
	top=18mm,
}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

% highlight text: \hl
\usepackage{soul}

\definecolor{hypecol}{HTML}{0875b7}
\hypersetup{%
    colorlinks,
    linkcolor={hypecol},
    citecolor={hypecol},
    urlcolor={hypecol}
}

\begin{document}

\maketitle

\begin{abstract}

    This is the abstract for a literature review

\end{abstract}

\section{Introduction}~\label{sec:introduction}

The generation of synthetic data is essential for various domains and tasks.
For example, synthetic data is used as a form of regularizing neural network
(\textit{i.e.}, data augmentation) \textbf{[CITATION]}. One form of
anonymizing datasets is via the production of synthetic observations
(\textit{i.e.}, synthetic data generation) \textbf{[CITATION]}. In settings
where only a small portion of training data is labeled, some techniques
generate artificial data using both labeled and unlabeled data with a modified
loss function to train neural networks (\textit{i.e.}, semi-supervised
learning)~\cite{laine2017temporal}. In imbalanced learning contexts, synthetic
data can be used to balance the target classes' frequencies, reinforcing the
learning of minority classes (\textit{i.e.},
oversampling)~\cite{fonseca2021improving}. Some active learning frameworks use
data generation to improve the quality of data selection and classifier
training~\cite{kim2021lada}. Other techniques employ data generation to
produce deep neural networks without labeled data (\textit{i.e.},
self-supervised learning)~\cite{grill2020bootstrap}.

Accordingly, the breadth of these techniques span multiple domains, such as
facial recognition~\cite{lv2017data}, Land Use/Land Cover mapping
\textbf{[CITATION]}, medical image processing \textbf{[CITATION]} and Natural
Language Processing~\cite{feng2021survey}. 

According to the domain, the data generation techniques used may vary
significantly. Generally speaking, there are data generation mechanisms
specific to most domains. However, multiple techniques are independent and can
be used for multiple data types, tasks or domains. \hl{For example, \ldots}

Depending on the context, evaluating the quality of the generated data is a
complex task. For example, for image and time series data, perceptually small
changes in the original data can lead to large changes in the euclidean
distance~\cite{assefa2020generating, theis2016note}.

In this literature review, we focus on the Machine Learning perspective of
synthetic data, as opposed to the practical perspective. From a practical
sense, synthetic data is used as a proxy of real data. It is assumed to be
inaccessible, essential and secondary for different purposes, such as
educational, software development, or systems
demonstrations~\cite{mannino2019real}.

\subsection{Contributions}

This literature review focuses on the generation mechanisms underlying the
different techniques where synthetic data is generated.

We focus particularly on tabular and feature space (\textit{i.e.}, embedded
inputs) augmentation given its breadth in scope.

Related literature reviews are mostly focused on specific algorithmic
applications, with little to no emphasis on the core generative process. For
this reason, these techniques often appear ``sandboxed'' even though there is
a significant overlap between them. 

The different taxonomies established are often specific to the technique
discussed. However, it is possible to establish a broader taxonomy without
giving up on specificity.

With this article, we aim to understand the current research gaps in the
different data mining techniques that involve synthetic data generation. We
compare the strengths and weaknesses of the models developed within each of
these fields. Finally, we identify possible future research directions to
address some of the limitations found.

Contributions of this paper are summarized below:

\begin{itemize}
    \item Bridge different ML concepts using synthetic data generation in its
        core \hl{(Algorithmic applications + Review of the State-of-the-art)}.
    \item List the different synthetic data generation/data augmentation
        taxonomies and characterize all relevant methods accordingly \hl{(Data
        augmentation taxonomy)}.
    \item Discuss the ML techniques in which synthetic data generation/data augmentation is used,
        beyond regularization and consolidate the current data generation
        mechanisms across the different techniques \hl{(Algorithmic Applications)}.
    \item Bring to light the key challenges of synthetic data generation and
        put forward possible research directions in the future.
\end{itemize}


% TODO: Develop research questions

\subsection{Paper Organization}

TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO

\section{Data Generation Taxonomy}

Image data augmentation taxonomy~\cite{khalifa2021comprehensive}

There is a distinction between semantic and traditional image data
augmentation~\cite{wang2021regularizing}, also discussed
in~\cite{shorten2019survey} 

Synthetic data generation for medical records
taxonomy~\cite{hernandez2022synthetic} which is incomplete



Data generation mechanisms can be characterized in 4 properties: Architecture,
Application level, Scope and Data space. The overall definition of the
proposed taxonomy is shown in Figure~\ref{fig:data-generation-taxonomy}.

\begin{enumerate}
    \item Level of application (External or Internal)
    \item Scope (Local or Global augmentation)
    \item Architectural approach (heuristic, network-based or others)
    \item Data space (Input, feature or output). Within feature and output: Domain
\end{enumerate}

\begin{figure}
	\centering
	\includegraphics[width=.8\linewidth]{../analysis/data-generation-taxonomy}
    \caption{General taxonomy of data generation mechanisms proposed in this
        paper.
    }~\label{fig:data-generation-taxonomy}
\end{figure}


\section{Synthetic Data Generation}
% Feature level data augmentation

According to~\cite{assefa2020generating}. The generation of synthetic data
should aim to fulfil the conditions below:

\begin{itemize}
    \item Privacy preserving.
    \item Human readable.
    \item Compact.
\end{itemize}

\section{Data Generation Mechanisms}
% Review of the State-of-the-art of domain specific data generation

In this section, we describe some popular domain and data type-specific data
generation techniques. For each data type we include a table with related
literature reviews specific to different domains.

\subsection{Tabular}
% TODO: 
% - Table with literature review references 

\subsection{Time series}
% TODO: 
% - Table with literature review references 

Generative adversarial networks in time series 

\subsection{Image}
% TODO: 
% - Table with literature review references 

Image-specific data generation mechanisms can be further divided into
traditional and semantic techniques~\cite{wang2021regularizing}. Traditional
generation techniques comprise simple modifications such as translation,
cropping or random erasing~\cite{zhong2017random}. Semantic generation methods
involve more complex tasks, such changing colors of specific attributes,
backgrounds and visual angles \textbf{[CITATION]}. 
% Go back to explaining wang2021regularizing

% semantic generation technique
Data generation by modifying specific attributes in data points with known
perturbations~\cite{lv2017data}. For example, overlaying facial elements into
a picture containing a human face (\textit{e.g.}, adding sunglasses and
different hairstyles), introducing perturbations in facial landmarks,
different illumination and artificial misalignment are different approaches to
generate artificial observations for facial recognition.

% GANs
Generative Adversarial Networks in computer vision~\cite{wang2021generative}

% traditional
% - mixup

\subsection{Text}
% TODO: 
% - Table with literature review references 

NLP also benefit from data augmentation~\cite{feng2021survey}.

In NLP, there is the challenge of establishing universal rules for text
transformations to provide new linguistic patterns~\cite{bayer2022data}

https://github.com/styfeng/DataAug4NLP

\section{Algorithmic applications}

\subsection{Data Privacy}

% Introduce synthetic data generation
Synthetic data generation is a technique used to produce synthetic, anonymized
versions of datasets~\cite{dankar2021fake}. It is considered a good approach
to share sensitive data without compromising significantly a given data mining
task~\cite{taub2018differential, park2018data}. Traditional data anonymization
techniques, as well as federated learning are two other viable solutions for
privacy-preserving data publishing tasks, but contain
drawbacks~\cite{hernandez2022synthetic}. On the one hand, traditional data
anonymization requires domain knowledge, is labor intensive and remains
susceptible to disclosure~\cite{reiter2004new}. On the other hand, federated
learning is a technically complex task that consists on training ML
classifiers on edge devices and aggregating temporarily updated parameters on
a centralized server, instead of aggregating the training
data~\cite{yu2022survey}. Although it prevents sharing sensitive data, its
applicability is dependent on the task. Dataset anonymization via synthetic
data generation attempts to balance disclosure risk and data utility in the
final synthetic dataset. The goal is to ensure observations are not
identifiable and the relevant data mining tasks are not
compromised~\cite{singh2017aggregating, li2018privacy}.

The generation of synthetic datasets allow a more flexible approach to the
successful implementation of ML tasks. However,

Anonymizing data using synthetic data generation in the financial
sector~\cite{assefa2020generating}.






\subsection{Regularization in Supervised Learning}

% Introduce data augmentation
The performance of Machine Learning models is highly dependent on the quality
of the training dataset used~\cite{Fenza2021, Halevy2009}. The presence of
imbalanced and/or small datasets, target labels incorrectly assigned, outliers
and high dimensional input spaces reduce the prospects of a successful machine
learning (ML) model implementation~\cite{Halevy2009, Domingos2012,
Salman2019}. In the case of deep learning, for example, these
models are often limited by a natural inclination to overfitting, label noise
memorization and catastrophic forgetting~\cite{Xie2021}. Regularization
methods are the typical approach to address these problems, but producing
robust ML solutions is still a challenge~\cite{Zhang2021}.

It is frequently assumed that the training data is sampled from a fixed data
source, it is balanced and does not contain label noise. Under these
conditions, the resulting ML classifier is expected to achieve good
generalization performance~\cite{benning2018modern}. Although, in practical
applications, this is rarely the case. When the training data is not
representative of the true population, or the model is over-parametrized, it
becomes particularly prone to overfitting~\cite{Bartlett2021}. Regularization
methods attempt to address these limitations. They can be divided into three
categories~\cite{santos2022avoiding}:

\begin{enumerate}
    \item Output level modifications. Transforms the labels in the training
        data.
    \item Algorithmic level modifications. Modifies the classifier's
        architecture, loss function or other components in the training
        procedure.
    \item Input level modifications. Modifies the training dataset by expanding it
        with synthetic data.
\end{enumerate}

The last approach, input level modifications, is known as data augmentation.
Data augmentation is used to increase the size and data variability of data in
a training dataset, by producing synthetic observations~\cite{Van2001,
Wong2016}. Since it is applied at the data level, it can be used for various
types of problems and classifiers~\cite{Behpour2019}. 

\subsection{Oversampling}

\subsection{Active Learning}

\subsection{Semi-supervised Learning}

\subsection{Self-supervised Learning}

\section{Discussion}

\subsection{Main Findings}

\subsubsection{RQ1: bla bla bla}

\subsubsection{RQ2: bla bla bla}

\subsubsection{RQ3: bla bla bla}

\subsection{Limitations}

Research across the different applications appears to be sandboxed even though
all techniques integrate synthetic data in its core.

% data privacy
The evaluation of anonymization techniques lack standardized, objective and
reliable performance metrics and benchmark datasets to allow an easier
comparison across classifiers to evaluate key aspects of data anonymization
(resemblance, utility, privacy and performance). These datasets should contain
mixed data types (\textit{i.e.}, a combination of categorical, ordinal,
continuous and discrete features) and the metrics should evaluate the
performance of different data mining tasks along with the anonymization
reliability.

% Regularization in supervised learning
Unlike with data privacy solutions, data augmentation techniques generally do
not consider the similarity/dissimilarity of synthetic data.

% oversampling
There is a lack of research on oversampling solutions to generate synthetic
data with mixed data types and datasets with exclusively non metric features.

% Active Learning

% Semi-supervised Learning

% Self-supervised Learning

% Societal challenges raised by data generation (?)
% Systematic bias

oversampling does not seem to be a relevant source of bias in behavioral
research and does not appear to have an appreciably different effect on
results for directly versus indirectly oversampled
variables~\cite{hauner2014latent}

% Maybe discuss adversarial attacks?

\subsection{Research directions}

% similarities between mixup and SMOTE, and an intermediate solution that can
% be further explored to use the mixup approach on tabular data
% (Geometric-SMOTE selection mechanism)

Quantifying the quality of the generated data:

\begin{enumerate}
    \item Realistic
    \item Similarity
    \item Usefulness (determine purpose and relevant performance metric)
    \item Understand the relationship between the 3 factors
\end{enumerate}

\section{Conclusions}

\bibliography{references}
\bibliographystyle{ieeetr}

\end{document}
